{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = os.listdir('../Data/Analysed')\n",
    "\n",
    "datas = []\n",
    "players = {}\n",
    "all_obs = [] \n",
    "games = []\n",
    "# playername, rating, difference, time \n",
    "for file in tqdm(files):\n",
    "    with open('../Data/Analysed/' + file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    white_moves_time, black_moves_time = data[\"game\"][\"moveTimestamps\"].split(\",\")[:-1][::2], data[\"game\"][\"moveTimestamps\"].split(\",\")[:-1][1::2]\n",
    "    white_moves_time, black_moves_time = [int(x)/10 for x in white_moves_time], [int(x)/10 for x in black_moves_time]\n",
    "    time_diffs_white, time_diffs_black = [0] + list(np.diff(white_moves_time)), [0] + list(np.diff(black_moves_time))\n",
    "    \n",
    "    user_black = data[\"players\"][\"top\"][\"username\"]\n",
    "    user_white = data[\"players\"][\"bottom\"][\"username\"]\n",
    "    rating_black = data[\"players\"][\"top\"][\"rating\"]\n",
    "    rating_white = data[\"players\"][\"bottom\"][\"rating\"]\n",
    "\n",
    "    white_loss = [x[\"difference\"] for x in data[\"evaluations\"] if x[\"player\"] == \"white\"]\n",
    "    black_loss = [x[\"difference\"] for x in data[\"evaluations\"] if x[\"player\"] == \"black\"]\n",
    "    white_loss_after_10 = np.median(white_loss[10:])\n",
    "    black_loss_after_10 = np.median(black_loss[10:])\n",
    "    white_loss = np.median(white_loss)\n",
    "    black_loss = np.median(black_loss)\n",
    "    white_perc_best_move = np.mean([x[\"Ranking Real Move\"] == 1 for x in data[\"evaluations\"] if x[\"player\"] == \"white\"])\n",
    "    black_perc_best_move = np.mean([x[\"Ranking Real Move\"] == 1 for x in data[\"evaluations\"] if x[\"player\"] == \"black\"])\n",
    "    white_perc_very_good_move = np.mean([x[\"Ranking Real Move\"] <= 3 for x in data[\"evaluations\"] if x[\"player\"] == \"white\"])\n",
    "    black_perc_very_good_move = np.mean([x[\"Ranking Real Move\"] <= 3 for x in data[\"evaluations\"] if x[\"player\"] == \"black\"])\n",
    "\n",
    "    white_perc_best_move_after_10 = np.mean([x[\"Ranking Real Move\"] == 1 for x in data[\"evaluations\"][10:] if x[\"player\"] == \"white\"])\n",
    "    black_perc_best_move_after_10 = np.mean([x[\"Ranking Real Move\"] == 1 for x in data[\"evaluations\"][10:] if x[\"player\"] == \"black\"])\n",
    "    # Ranking Real Move (if -1 => 10)\n",
    "    avg_rank_white = np.mean([x[\"Ranking Real Move\"] if x[\"Ranking Real Move\"] > 0 else 10 for x in data[\"evaluations\"] if x[\"player\"] == \"white\"])\n",
    "    avg_rank_black = np.mean([x[\"Ranking Real Move\"] if x[\"Ranking Real Move\"] > 0 else 10 for x in data[\"evaluations\"] if x[\"player\"] == \"black\"])\n",
    "    ranks_white = [x[\"Ranking Real Move\"] if x[\"Ranking Real Move\"] > 0 else 10 for x in data[\"evaluations\"] if x[\"player\"] == \"white\"]\n",
    "    ranks_black = [x[\"Ranking Real Move\"] if x[\"Ranking Real Move\"] > 0 else 10 for x in data[\"evaluations\"] if x[\"player\"] == \"black\"]\n",
    "\n",
    "    if user_black not in players:\n",
    "        players[user_black] = {\"games\": 0, \"loss\": [], \"loss_after_10\": [], \"perc_best_move\": [], \"perc_best_move_after_10\": [], \"rating\": [], \"avg_rank\": [], \"ranks\": [], \"very_good_move\": []}\n",
    "    if user_white not in players:\n",
    "        players[user_white] = {\"games\": 0, \"loss\": [], \"loss_after_10\": [], \"perc_best_move\": [], \"perc_best_move_after_10\": [], \"rating\": [], \"avg_rank\": [], \"ranks\": [], \"very_good_move\": []}\n",
    "    \n",
    "    players[user_black][\"games\"] += 1\n",
    "    players[user_white][\"games\"] += 1\n",
    "    players[user_black][\"loss\"].append(black_loss)\n",
    "    players[user_white][\"loss\"].append(white_loss)\n",
    "    players[user_black][\"loss_after_10\"].append(black_loss_after_10)\n",
    "    players[user_white][\"loss_after_10\"].append(white_loss_after_10)\n",
    "    players[user_black][\"perc_best_move\"].append(black_perc_best_move)\n",
    "    players[user_white][\"perc_best_move\"].append(white_perc_best_move)\n",
    "    players[user_black][\"perc_best_move_after_10\"].append(black_perc_best_move_after_10)\n",
    "    players[user_white][\"perc_best_move_after_10\"].append(white_perc_best_move_after_10)\n",
    "    players[user_black][\"rating\"].append(rating_black)\n",
    "    players[user_white][\"rating\"].append(rating_white)\n",
    "    players[user_black][\"avg_rank\"].append(avg_rank_black)\n",
    "    players[user_white][\"avg_rank\"].append(avg_rank_white)\n",
    "    players[user_black][\"ranks\"].append(ranks_black)\n",
    "    players[user_white][\"ranks\"].append(ranks_white)\n",
    "    players[user_black][\"very_good_move\"].append(black_perc_very_good_move)\n",
    "    players[user_white][\"very_good_move\"].append(white_perc_very_good_move)\n",
    "\n",
    "    white_moves, black_moves = 0, 0\n",
    "    for i in range(len(data[\"evaluations\"])):\n",
    "        white_moves += 1 if data[\"evaluations\"][i][\"player\"] == \"white\" else 0\n",
    "        black_moves += 1 if data[\"evaluations\"][i][\"player\"] == \"black\" else 0\n",
    "        all_obs.append({\"player\": user_black if i % 2 == 0 else user_white, \n",
    "                        \"colour\": 0 if i % 2 == 0 else 1,\n",
    "                        \"rating\":  np.mean(players[user_black][\"rating\"]) if i % 2 == 0 else np.mean(players[user_white][\"rating\"]),\n",
    "                        \"difference\": -np.abs(data[\"evaluations\"][i][\"Best Move Eval\"] - data[\"evaluations\"][i][\"Real Move Eval\"]),\n",
    "                        \"time\": time_diffs_white[white_moves-1] if data[\"evaluations\"][i][\"player\"] == \"white\" else time_diffs_black[black_moves-1],\n",
    "                        \"remaining_time\": white_moves_time[white_moves-1] if data[\"evaluations\"][i][\"player\"] == \"white\" else black_moves_time[black_moves-1],\n",
    "                        \"Ranking Real Move\": data[\"evaluations\"][i][\"Ranking Real Move\"],\n",
    "                        \"Move_Num\" : white_moves if data[\"evaluations\"][i][\"player\"] == \"white\" else black_moves,\n",
    "                        \"opponent\": user_white if i % 2 == 0 else user_black,\n",
    "                        \"game_id\": data[\"game\"][\"id\"],\n",
    "                        \"oppenent_rating\": np.mean(players[user_white][\"rating\"]) if i % 2 == 0 else np.mean(players[user_black][\"rating\"]),\n",
    "                        \"real_eval\": data[\"evaluations\"][i][\"Real Move Eval\"],\n",
    "                        \"best_eval\": data[\"evaluations\"][i][\"Best Move Eval\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_sequences = {}\n",
    "ratings = {}\n",
    "for obs in all_obs:\n",
    "    if f\"\"\"{obs[\"game_id\"]}_{obs[\"player\"]}\"\"\" not in games_sequences:\n",
    "        games_sequences[f\"\"\"{obs[\"game_id\"]}_{obs[\"player\"]}\"\"\"] = []\n",
    "    games_sequences[f\"\"\"{obs[\"game_id\"]}_{obs[\"player\"]}\"\"\"].append([obs[\"time\"],obs[\"Ranking Real Move\"], obs[\"real_eval\"],  obs[\"best_eval\"], obs[\"colour\"], obs[\"difference\"], obs[\"remaining_time\"]])\n",
    "#obs[\"difference\"], \n",
    "\n",
    "    if f\"\"\"{obs[\"game_id\"]}_{obs[\"player\"]}\"\"\" not in ratings:\n",
    "        ratings[f\"\"\"{obs[\"game_id\"]}_{obs[\"player\"]}\"\"\"] = obs[\"rating\"]\n",
    "\n",
    "print(len(games_sequences))\n",
    "games_sequences = {k: v for k, v in games_sequences.items() if len(v) > 20}\n",
    "print(len(games_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# Functions for normalization and denormalization\n",
    "def normalize(y):\n",
    "    min_val = np.min(y)\n",
    "    max_val = np.max(y)\n",
    "    y_normalized = (y - min_val) / (max_val - min_val)\n",
    "    return y_normalized, min_val, max_val\n",
    "\n",
    "def denormalize(y_normalized, min_val, max_val):\n",
    "    y_original = y_normalized * (max_val - min_val) + min_val\n",
    "    return y_original\n",
    "\n",
    "adjusted_sequences = {key: val[0:40] for key, val in games_sequences.items()}\n",
    "\n",
    "# Ensure that the sequences in X and the ratings in y match up\n",
    "X = []\n",
    "y = []\n",
    "for key in adjusted_sequences:\n",
    "    X.append(adjusted_sequences[key])\n",
    "    y.append(ratings[key])\n",
    "\n",
    "# Padding the sequences to a fixed length of 30\n",
    "X_padded = pad_sequences(X, maxlen=30, padding='post', dtype='float32')\n",
    "# Normalize the ratings (y-values)\n",
    "y_normalized, min_val, max_val = normalize(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMetricsCallback(Callback):\n",
    "    def __init__(self, X_data, y_data, min_val, max_val, n_splits=5):\n",
    "        super().__init__()\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "        self.kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        self.split_gen = self.kf.split(self.X_data)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        try:\n",
    "            _, test_index = next(self.split_gen)\n",
    "        except StopIteration:\n",
    "            self.split_gen = self.kf.split(self.X_data)\n",
    "            _, test_index = next(self.split_gen)\n",
    "\n",
    "        X_test, y_test_normalized = self.X_data[test_index], self.y_data[test_index]\n",
    "        y_pred_normalized = self.model.predict(X_test)\n",
    "        \n",
    "        # Denormalize predictions and ground truth\n",
    "        y_pred = denormalize(y_pred_normalized, self.min_val, self.max_val)\n",
    "        y_test = denormalize(y_test_normalized, self.min_val, self.max_val)\n",
    "\n",
    "        avg_pred = np.mean(y_pred)\n",
    "        std_pred = np.std(y_pred)\n",
    "        mae = np.mean(np.abs(y_pred - y_test))\n",
    "        mse = np.mean((y_pred - y_test)**2)\n",
    "        mad = np.mean(np.abs(y_pred - y_test))\n",
    "\n",
    "        print(\"\\nAvg Pred: {:.4f} | Std Pred: {:.4f} | MAE: {:.4f} | MSE: {:.4f} | MAD: {:.4f}\".format(avg_pred, std_pred, mae, mse, mad))\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(30, 7), return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "custom_metrics = CustomMetricsCallback(X_data=X_padded, y_data=y_normalized, min_val=min_val, max_val=max_val)\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.fit(X_padded, y_normalized, \n",
    "          validation_data=(X_padded, y_normalized),  # Placeholder, not actually used for validation.\n",
    "          epochs=100, \n",
    "          batch_size=16, \n",
    "          callbacks=[custom_metrics], \n",
    "          verbose=1)\n",
    "\n",
    "# Write Model to Disk in ../Model/\n",
    "model.save('../Model/elo_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../Model/elo_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_normalized = model.predict(X_padded)\n",
    "y_pred = denormalize(y_pred_normalized, min_val, max_val)\n",
    "y_original = denormalize(y_normalized, min_val, max_val)\n",
    "\n",
    "plt.scatter(y_original, y_pred, s = 10, edgecolor='black')\n",
    "# Add 45 degree line\n",
    "plt.plot([min(y_original), max(y_original)], [min(y_original), max(y_original)], 'k--', lw=3, color='red')\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Predictions vs Ground Truth. Elo Rating\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
